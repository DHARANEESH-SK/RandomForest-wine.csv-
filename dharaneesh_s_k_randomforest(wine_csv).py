# -*- coding: utf-8 -*-
"""Dharaneesh S K Randomforest(wine.csv).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bqo_oLgTChZ3CsQsuRSah3noIl2U0uII
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc

#Visualize the first 10 rows of the data set
data = pd.read_csv("/content/drive/MyDrive/0.MKCE/5.Random Forest/3 Take-Home Assignment/wine.csv")
data.head(10)

#check the shape of dataset
data.shape

# Show distribution of the numerical coloumn
fig = data.hist(figsize = (12,12),color='green')
plt.show()

#Generate the correlation matrix
correlation = data.corr()
print(correlation)

data.corr()["quality"]

#Generate pair-plot for the data
sns.pairplot(data)

#Generate a count plot for the target variable (quality)
sns.countplot(x='quality',data=data)

#Converting the target variable 'Quality' to categorical, such that
#Wines having the “Quality” value > 6.5 are assigned value 1, and
#Wines having the “Quality” value < 6.5, are assigned value 0
#Where 0: Ordinary Quality of wine and 1: High quality of wine
quality = data['quality']
values = []
for k in quality:
  if(k<6.5):
    j="Ordinary Quality of wine"
    values.append(j)
  elif(k>6.5):
    j="High quality of wine"
    values.append(j)
values

#creating a dataframe to save the wine quality
k=pd.DataFrame(values)
data['Wine_Quality']=k
data

#Build a Random Forest classifier, to predict whether a particular wine is ordinary or of high quality. 
#Perform Hyperparameter Tuning to improve the accuracy of the model.
from google.colab import drive
drive.mount('/content/drive')

#importing libraries for gridsearch
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc

#to read the dataset
data = pd.read_csv("/content/drive/MyDrive/0.MKCE/5.Random Forest/3 Take-Home Assignment/wine.csv")
data.head()

data.info()

data.columns

#creating dummies 
data2 = pd.get_dummies(data)
data2.head()

#drop the label from x to y
x= data2.drop(['quality',],axis=1).values
y= data2['quality'].values

#print the data2
data2

#train test split
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.1,random_state=0)

#shapes of x,y(train , test)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#create a randomforestclassifier model
model = RandomForestClassifier()
model.fit(x_train,y_train)

#predicted
pre = model.predict(x_test)

#probability of predicted model
model.predict_proba(x_test)

#accuracy
accuracy_score(y_test,pre)

#confusion matrix
confusion_matrix(y_test,pre)

#result
quality= classification_report(y_test,pre)
print(quality)

from sklearn.tree import DecisionTreeClassifier
from sklearn import decomposition,datasets
from sklearn import tree
from sklearn import ensemble
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler

sc= StandardScaler()
pca = decomposition.PCA()
randomforest = ensemble.RandomForestClassifier()
#creating a Pipelilne
pipe = Pipeline(steps = [('sc',sc),
                         ('pca',pca),
                        ('randomforest',randomforest) ])
#creating a range for pipeline
n_components = list(range(1,x.shape[1]+1,1))

criterion =['gini','entropy']
max_depth = [2,4,6,8]

parameters = dict(pca__n_components=n_components,
                  randomforest__criterion=criterion,
                  randomforest__max_depth=max_depth)

clf = GridSearchCV(pipe,parameters)

clf.fit(x,y)

print('Best Criterion:',clf.best_estimator_.get_params()['randomforest__criterion'])

print('Best max_depth:',clf.best_estimator_.get_params()['randomforest__max_depth'])

print('Best no.of components:',clf.best_estimator_.get_params()['pca__n_components'])

print(clf.best_estimator_.get_params()['randomforest'])

CV_result = cross_val_score(clf,x,y,cv=4,n_jobs=-1)

print(CV_result)
print(CV_result.mean())
print(CV_result.std())
